{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f78d3b9e",
      "metadata": {
        "id": "0259a7ce8120"
      },
      "source": [
        "# Create, train, and deploy a text classification model on Vertex AI\n",
        "\n",
        "This notebook walks you through the major phases of building and using a text classification model on Vertex AI. Specifically, this notebook demonstrates how to:\n",
        "\n",
        "* Set up your development environment\n",
        "* Create a dataset and import data\n",
        "* Train a model\n",
        "* Get and review evaluations for the model\n",
        "* Deploy a model to an endpoint\n",
        "* Get online predictions\n",
        "* Get batch predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb45460a",
      "metadata": {
        "id": "a5cb73702a9b"
      },
      "source": [
        "## Set up your development environment\n",
        "\n",
        "This notebook uses the Python SDK for Vertex AI, which is contained in the `python-aiplatform` package. You must first install the package into your development environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86f59ba3",
      "metadata": {
        "id": "8a4bc7ca60ee"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# The Google Cloud Notebook product has specific requirements\n",
        "IS_GOOGLE_CLOUD_NOTEBOOK = os.path.exists(\"/opt/deeplearning/metadata/env_version\")\n",
        "\n",
        "# Google Cloud Notebook requires dependencies to be installed with '--user'\n",
        "USER_FLAG = \"\"\n",
        "if IS_GOOGLE_CLOUD_NOTEBOOK:\n",
        "    USER_FLAG = \"--user\"\n",
        "\n",
        "! pip3 install {USER_FLAG} --upgrade google-cloud-aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3fe8f3d",
      "metadata": {
        "id": "dab929a288b0"
      },
      "source": [
        "Next, you need to import the package into your development environment. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06195ee7",
      "metadata": {
        "id": "962cc1de4da7"
      },
      "outputs": [],
      "source": [
        "from google.cloud import aiplatform"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ce3ba04",
      "metadata": {
        "id": "9063a252e22a"
      },
      "source": [
        "Finally, you must initialize the client library before you can send requests to the Vertex AI service. With the Python SDK, you initialize the client library as shown in the following cell. Be sure to provide the ID for your Google Cloud project in the `project` variable.\n",
        "\n",
        "This notebook uses the `us-central1` Compute location, although you can change it to another location. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1e7b2e",
      "metadata": {
        "id": "aad66f254ed4"
      },
      "outputs": [],
      "source": [
        "project = \"[your-project-id]\"\n",
        "location = \"us-central1\"\n",
        "\n",
        "aiplatform.init(project=project, location=location)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a16bde8",
      "metadata": {
        "id": "32c971919605"
      },
      "source": [
        "## Create a dataset and import your data\n",
        "\n",
        "The notebook uses the 'Happy Moments' dataset for demonstration purposes. You can change it to another text classification dataset that [conforms to the data preparation requirements](https://cloud.google.com/vertex-ai/docs/datasets/prepare-text#classification).\n",
        "\n",
        "Using the Python SDK, you can create a dataset and import the dataset in one call to `TextDataset.create()`, as shown in the following cell.\n",
        "\n",
        "This next step can take a while. The client library prints out statements that include the name of the new dataset--you might want to copy the resource name down someplace.\n",
        "\n",
        "**Note**: You can close this tab while you wait for this operation to complete. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72f24754",
      "metadata": {
        "id": "6caf82e5e84e"
      },
      "outputs": [],
      "source": [
        "src_uris = \"gs://cloud-ml-data/NL-classification/happiness.csv\"\n",
        "display_name = \"e2e-text-dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9e2777",
      "metadata": {
        "id": "d35b8b6b94ae"
      },
      "outputs": [],
      "source": [
        "ds = aiplatform.TextDataset.create(\n",
        "    display_name=display_name,\n",
        "    gcs_source=src_uris,\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.text.single_label_classification,\n",
        "    sync=True,\n",
        ")\n",
        "\n",
        "ds.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5b96fe4",
      "metadata": {
        "id": "5b3cc427353a"
      },
      "source": [
        "## Train your text classification model\n",
        "\n",
        "Once your dataset has finished importing data, you are ready to train your model. To do this, you first need the full resource name of your dataset, where the full name has the format `projects/[YOUR_PROJECT]/locations/us-central1/datasets/[YOUR_DATASET_ID]`. If you don't have the resource name handy, you can list all of the datasets in your project using `TextDataset.list()`. \n",
        "\n",
        "As shown in the following code block, you can pass in the display name of your dataset in the call to `list()` to filter the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb7f3d2f",
      "metadata": {
        "id": "52cf56f1c8a9"
      },
      "outputs": [],
      "source": [
        "aiplatform.TextDataset.list(filter=f'display_name=\"{display_name}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b7eef0f",
      "metadata": {
        "id": "68f10356cab9"
      },
      "source": [
        "Now you can begin training your model. Training the model is a two part process:\n",
        "\n",
        "1. **Define the training job.** You must provide a display name and the type of training you want when you define the training job.\n",
        "2. **Run the training job.** When you run the training job, you need to supply a reference to the dataset to use for training. At this step, you can also configure the train/test/validate split percentages.\n",
        "\n",
        "You do not need to specify train/test/validate splits. The training job has a default setting of 80%/10%/10% if you don't provide these values.\n",
        "\n",
        "As with importing data into the dataset, training your model can take a substantial amount of time. The client library prints out operation status messages while the training pipeline operation processes. You must wait for the training process to complete before you can get the resource name and ID of your new model.\n",
        "\n",
        "**Note**: You can close this tab while you wait for the operation to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c242d4",
      "metadata": {
        "id": "0aa0f01805ea"
      },
      "outputs": [],
      "source": [
        "# Define the training job\n",
        "training_job_display_name = \"e2e-text-training-job\"\n",
        "job = aiplatform.AutoMLTextTrainingJob(\n",
        "    display_name=training_job_display_name,\n",
        "    prediction_type=\"classification\",\n",
        "    multi_label=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b0a813",
      "metadata": {
        "id": "1ec60baf2c51"
      },
      "outputs": [],
      "source": [
        "# Run the training job\n",
        "dataset_id = \"[your-dataset-id]\"\n",
        "model_display_name = \"e2e-text-classification-model\"\n",
        "\n",
        "text_dataset = aiplatform.TextDataset(dataset_id)\n",
        "\n",
        "model = job.run(\n",
        "    dataset=text_dataset,\n",
        "    model_display_name=model_display_name,\n",
        "    training_fraction_split=0.7,\n",
        "    validation_fraction_split=0.2,\n",
        "    test_fraction_split=0.1,\n",
        "    sync=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "124ea5cf",
      "metadata": {
        "id": "caaa3f32b12e"
      },
      "source": [
        "## Get and review model evaluation scores\n",
        "\n",
        "After your model has finished training, you can review the evaluation scores for it.\n",
        "\n",
        "First, you need to get the resource name of your new model. To get the resource name, list all of the models in your project. As before with datasets, you can provide filter criteria to narrow down your search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "937c957d",
      "metadata": {
        "id": "b0bb6be8621a"
      },
      "outputs": [],
      "source": [
        "aiplatform.Model.list(filter='display_name=\"e2e-text-classification-model\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58e2c9d9",
      "metadata": {
        "id": "8481b6878ed2"
      },
      "source": [
        "Using the model name (in the format `projects/[PROJECT_NAME]/locations/us-central1/models/[MODEL_ID`), you can get its model evaluations. To get model evaluations, you must use the underlying service client.\n",
        "\n",
        "Building a service client requires that you provide the name of the regionalized hostname used for your model. In this tutorial, the hostname is `us-central1-aiplatform.googleapis.com` because the model was created in the `us-central1` location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50e46025",
      "metadata": {
        "id": "a8443fc8861f"
      },
      "outputs": [],
      "source": [
        "model_name = \"[your-model-id]\"\n",
        "client_options = {\"api_endpoint\": \"us-central1-aiplatform.googleapis.com\"}\n",
        "model_service_client = aiplatform.gapic.ModelServiceClient(\n",
        "    client_options=client_options\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89e90d14",
      "metadata": {
        "id": "b8a788593609"
      },
      "source": [
        "Before you can view the model evaluation you must first list all of the evaluations for that model. Each model can have multiple evaluations, although new model is likely to only have one. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec598cb",
      "metadata": {
        "id": "fdcb045e29f2"
      },
      "outputs": [],
      "source": [
        "model_evaluations = model_service_client.list_model_evaluations(parent=model_name)\n",
        "model_evaluation = list(model_evaluations)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf26190",
      "metadata": {
        "id": "cd7d3afae05c"
      },
      "source": [
        "Now that you have the model evaluation, you can look at your model's scores. If you have questions about what the scores mean, review the [public documentation](https://cloud.google.com/vertex-ai/docs/training/evaluating-automl-models#text).\n",
        "\n",
        "The results returned from the service are formatted as [`google.protobuf.Value`](https://googleapis.dev/python/protobuf/latest/google/protobuf/struct_pb2.html) objects. You can transform the return object as a `dict` for easier reading and parsing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bde46d2",
      "metadata": {
        "id": "6eb9ccb0a0a0"
      },
      "outputs": [],
      "source": [
        "from google.protobuf import json_format\n",
        "\n",
        "model_eval_dict = json_format.MessageToDict(model_evaluation._pb)\n",
        "metrics = model_eval_dict[\"metrics\"]\n",
        "confidence_metrics = metrics[\"confidenceMetrics\"]\n",
        "\n",
        "print(f'Area under precision-recall curve (AuPRC): {metrics[\"auPrc\"]}')\n",
        "for confidence_scores in confidence_metrics:\n",
        "    metrics = confidence_scores.keys()\n",
        "    print(\"\\n\")\n",
        "    for metric in metrics:\n",
        "        print(f\"\\t{metric}: {confidence_scores[metric]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de5c87b9",
      "metadata": {
        "id": "b5dbe4dbaa60"
      },
      "source": [
        "## Deploy your text classification model\n",
        "\n",
        "Once your model has completed training, you must deploy it to an _endpoint_ to get online predictions from it. When you deploy the model to an endpoint, a copy of the model is made on the endpoint with a new resource name and display name.\n",
        "\n",
        "You can deploy multiple models to the same endpoint and split traffic between the various models assigned to the endpoint. However, you must deploy one model at a time to the endpoint. To change the traffic split percentages, you must assign new values on your second (and subsequent) models each time you deploy a new model.\n",
        "\n",
        "The following code block demonstrates how to deploy a model. The code snippet relies on the Python SDK to create a new endpoint for deployment. During deployment, the Python SDK prints out the name of your new endpoint--you might want to record the name of the endpoint for future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e97df845",
      "metadata": {
        "id": "19bc4a55ccfe"
      },
      "outputs": [],
      "source": [
        "deployed_model_display_name = \"e2e-deployed-text-classification-model\"\n",
        "\n",
        "model.deploy(deployed_model_display_name=deployed_model_display_name, sync=True)\n",
        "\n",
        "model.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75630611",
      "metadata": {
        "id": "531da446035b"
      },
      "source": [
        "In case you didn't record the name of the new endpoint, you can get a list of all your endpoints as you did before with datasets and models. For each endpoint, you can list the models deployed to that endpoint. To get a reference to the model that you just deployed, you check the `display_name` of each model to the models deployed to each endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a405e052",
      "metadata": {
        "id": "f61fb44181b4"
      },
      "outputs": [],
      "source": [
        "endpoints = aiplatform.Endpoint.list()\n",
        "\n",
        "endpoint_with_deployed_model = []\n",
        "\n",
        "for endpoint in endpoints:\n",
        "    for model in endpoint.list_models():\n",
        "        if model.display_name.find(deployed_model_display_name) == 0:\n",
        "            endpoint_with_deployed_model.append(endpoint)\n",
        "\n",
        "print(endpoint_with_deployed_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9873570d",
      "metadata": {
        "id": "351a6e8be3a5"
      },
      "source": [
        "## Get online predictions from your model\n",
        "\n",
        "Now that you have your endpoint's resource name, you can get online predictions from the text classification model. To get the online prediction, you send a prediction request to your endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "859cbbe5",
      "metadata": {
        "id": "953b333fc0fc"
      },
      "outputs": [],
      "source": [
        "endpoint_name = \"[your-endpoint-id]\"\n",
        "endpoint = aiplatform.Endpoint(endpoint_name)\n",
        "content = \"I got a high score on my math final!\"\n",
        "\n",
        "response = endpoint.predict(instances=[{\"content\": content}])\n",
        "\n",
        "for prediction_ in response.predictions:\n",
        "    ids = prediction_[\"ids\"]\n",
        "    display_names = prediction_[\"displayNames\"]\n",
        "    confidence_scores = prediction_[\"confidences\"]\n",
        "    for count, id in enumerate(ids):\n",
        "        print(f\"Prediction ID: {id}\")\n",
        "        print(f\"Prediction display name: {display_names[count]}\")\n",
        "        print(f\"Prediction confidence score: {confidence_scores[count]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1985a3ce",
      "metadata": {
        "id": "f18811cd0477"
      },
      "source": [
        "## Get batch predictions from your model\n",
        "\n",
        "You can get batch predictions from a text classification model without deploying it. You must first format all of your prediction instances (prediction input) in JSONL format and you must store the JSONL file in a Google Cloud Storage bucket. You must also provide a Google Cloud Storage bucket to hold your prediction output.\n",
        "\n",
        "To start, you must first create your predictions input file in JSONL format. Each line in the JSONL document needs to be formatted like so:\n",
        "\n",
        "```\n",
        "{ \"content\": \"gs://sourcebucket/datasets/texts/source_text.txt\", \"mimeType\": \"text/plain\"}\n",
        "```\n",
        "\n",
        "The `content` field in the JSON structure must be a Google Cloud Storage URI to another document that contains the text input for prediction.\n",
        "[See the documentation for more information.](https://cloud.google.com/ai-platform-unified/docs/predictions/batch-predictions#text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae17ff8c",
      "metadata": {
        "id": "e4b838cbcd99"
      },
      "outputs": [],
      "source": [
        "instances = [\n",
        "    \"We hiked through the woods and up the hill to the ice caves\",\n",
        "    \"My kitten is so cute\",\n",
        "]\n",
        "input_file_name = \"batch-prediction-input.jsonl\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48833f69",
      "metadata": {
        "id": "76ac422ab8dd"
      },
      "source": [
        "For this tutorial, you can create a new set of input files using an existing Google Cloud Storage bucket. You need to provide the URI for the bucket in your request. For batch prediction, you must use a standard regional bucket."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4cfb5cc",
      "metadata": {
        "id": "1e0759fbb219"
      },
      "outputs": [],
      "source": [
        "BUCKET_NAME = \"gs://[your-bucket-name]\"  # @param {type:\"string\"}\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "\n",
        "if BUCKET_NAME == \"\" or BUCKET_NAME is None or BUCKET_NAME == \"gs://[your-bucket-name]\":\n",
        "    BUCKET_NAME = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n",
        "\n",
        "gcs_uri = f\"gs://{gcs_bucket_uri}/{gcs_prefix}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdc1017c",
      "metadata": {
        "id": "5db82aaf03e4"
      },
      "outputs": [],
      "source": [
        "! gsutil mb -l $REGION $BUCKET_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85a63e20",
      "metadata": {
        "id": "8b7cabbb86ad"
      },
      "outputs": [],
      "source": [
        "input_file_data = []\n",
        "input_str = \"\"\n",
        "for count, instance in enumerate(instances):\n",
        "    tmp_data = {\"content\": f\"{gcs_uri}/input_{count}.txt\", \"mimeType\": \"text/plain\"}\n",
        "    input_file_data.append(tmp_data)\n",
        "    blob = bucket.blob(f\"{gcs_prefix}/input_{count}.txt\")\n",
        "    blob.upload_from_string(instance)\n",
        "    input_str += str(tmp_data) + \"\\n\"\n",
        "\n",
        "print(input_file_data)\n",
        "\n",
        "file_blob = bucket.blob(f\"{gcs_prefix}/{input_file_name}\")\n",
        "file_blob.upload_from_string(input_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d485446a",
      "metadata": {
        "id": "f5ab2139d52d"
      },
      "outputs": [],
      "source": [
        "job_display_name = \"e2s-batch-prediction-job\"\n",
        "model = aiplatform.Model(model_name=model_name)\n",
        "\n",
        "batch_prediction_job = model.batch_predict(\n",
        "    job_display_name=job_display_name,\n",
        "    gcs_source=f\"{gcs_uri}/{input_file_name}\",\n",
        "    gcs_destination_prefix=gcs_uri,\n",
        "    sync=True,\n",
        ")\n",
        "\n",
        "batch_prediction_job.wait()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e15660586fb"
      },
      "source": [
        "Once you have finished reviewing the output from your batch prediction job, you can delete the bucket that you created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c74b687a8b3d"
      },
      "outputs": [],
      "source": [
        "! gsutil rb $BUCKET_NAME"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "automl-text-classification.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
